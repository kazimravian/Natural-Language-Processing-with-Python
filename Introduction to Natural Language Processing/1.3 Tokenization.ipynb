{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["<h1><b><font color = 'brown'>\n","Tokenization\n","</font></b></h1>"],"metadata":{"id":"1PzueVZ7Camg"}},{"cell_type":"markdown","source":["<h2>\n","<b>\n","\n","<ul>\n","<font color = 'brown green'>\n","\n","<li>\n","Tokenization refers to the procedure of splitting a sentence into its constituent parts—the words and punctuation that it is made up of.\n","</li><br>\n","\n","<li>\n","It is different from simply splitting the sentence on whitespaces, and instead actually divides the sentence into constituent words, numbers (if any), and punctuation, which may not always be separated by whitespaces.\n","</li><br>\n","\n","<li>\n","For example, consider this sentence: \"I am reading a book.\" Here, our task is to extract words/tokens from this sentence. After passing this sentence to a tokenization program, the extracted words/tokens would be \"I,\" \"am,\" \"reading,\" \"a,\" \"book,\" and \".\" – this example extracts one token at a time. Such tokens\n","are called unigrams.\n","</li><br>\n","\n","<li>\n","To get a better understanding of tokenization, let's solve an exercise based on it.\n","</li><br>\n","\n","</ul>\n","</b>\n","</h2>"],"metadata":{"id":"v8HCsxkACqKc"}},{"cell_type":"markdown","source":["<h1>\n","<b>\n","<font color = 'brown'>\n","Exercise 2: Tokenization of a Simple Sentence\n","</font>\n","</b>\n","</h1>"],"metadata":{"id":"tOeUSxfqEXOu"}},{"cell_type":"markdown","source":["<h2>\n","<b>\n","\n","<ul>\n","<font color = 'brown green'>\n","\n","<li>\n","In this exercise, we will tokenize the words in a given sentence with the help of the NLTK library. Follow these steps to implement this exercise using the sentence, \"I am reading NLP Fundamentals.\"\n","</li><br>\n","\n","</ul>\n","</b>\n","</h2>"],"metadata":{"id":"ypbBsZvmEjaY"}},{"cell_type":"markdown","source":["1. Open a Jupyter Notebook."],"metadata":{"id":"Skemy-r0E1kk"}},{"cell_type":"markdown","source":["2. Insert a new cell and add the following code to import the necessary libraries and download the different types of NLTK data that we are going to use for different tasks in the following exercises:"],"metadata":{"id":"2L7x7VoEE7Zf"}},{"cell_type":"code","source":["from nltk import word_tokenize, download\n","download(['punkt','averaged_perceptron_tagger','stopwords'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qfY0-SaDCi4z","executionInfo":{"status":"ok","timestamp":1671029719367,"user_tz":-300,"elapsed":714,"user":{"displayName":"Kazim Ali","userId":"17788360296826308952"}},"outputId":"45448990-6a31-4960-dcff-09f63a47b71d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["3. The **word_tokenize()** method is used to split the sentence into words/\n","tokens. We need to add a sentence as input to the **word_tokenize()** method\n","so that it performs its job. The result obtained will be a list, which we will store in a **word** variable. To implement this, insert a new cell and add the following code:"],"metadata":{"id":"EplcEqO8FEbA"}},{"cell_type":"code","source":["def get_tokens(sentence):\n","  words = word_tokenize(sentence)\n","  return words"],"metadata":{"id":"MFPdzjCAFgf7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["4. In order to view the list of tokens generated, we need to view it using the **print()** function. Insert a new cell and add the following code to implement this:"],"metadata":{"id":"qR6sbBTDFhov"}},{"cell_type":"code","source":["print(get_tokens(\"I am reading NLP Fundamentals.\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DJyEN2FuFuWC","executionInfo":{"status":"ok","timestamp":1671029734603,"user_tz":-300,"elapsed":4,"user":{"displayName":"Kazim Ali","userId":"17788360296826308952"}},"outputId":"2f474d04-8627-498b-d7ab-c2fe15af46e9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['I', 'am', 'reading', 'NLP', 'Fundamentals', '.']\n"]}]}]}